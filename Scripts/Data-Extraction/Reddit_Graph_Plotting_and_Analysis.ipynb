{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sun\n",
      "[nltk_data]     Coliamco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sun Coliamco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Sun\n",
      "[nltk_data]     Coliamco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sun\n",
      "[nltk_data]     Coliamco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Sun\n",
      "[nltk_data]     Coliamco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import bigrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the files in this block\n",
    "\n",
    "data_frame_test = pd.read_csv(\"../../Data/Analysis_ready/clean_test_reddit.csv\")\n",
    "# data_frame_odi = pd.read_csv(None)\n",
    "# data_frame_t20 = pd.read_csv(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post_id</th>\n",
       "      <th>Id</th>\n",
       "      <th>Time</th>\n",
       "      <th>Author</th>\n",
       "      <th>Score</th>\n",
       "      <th>No_comments</th>\n",
       "      <th>Body</th>\n",
       "      <th>Type</th>\n",
       "      <th>Pre-processed_body</th>\n",
       "      <th>Vader_Pre-processed_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>1f0syen</td>\n",
       "      <td>2024-08-25 20:04:21</td>\n",
       "      <td>NoQuestion4045</td>\n",
       "      <td>2445</td>\n",
       "      <td>281</td>\n",
       "      <td>Bangladesh beats Pakistan for the first ever i...</td>\n",
       "      <td>Post</td>\n",
       "      <td>bangladesh beat pakistan first ever test cricket</td>\n",
       "      <td>Bangladesh beats Pakistan for the first ever i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>lju4ld6</td>\n",
       "      <td>2024-08-25 20:07:28</td>\n",
       "      <td>deleted</td>\n",
       "      <td>1151</td>\n",
       "      <td>0</td>\n",
       "      <td>Fort Rawalpindi, yet to be captured, by the ho...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>fort rawalpindi yet capture home side</td>\n",
       "      <td>Fort Rawalpindi , yet to be captured , by the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>lju9bzy</td>\n",
       "      <td>2024-08-25 20:58:53</td>\n",
       "      <td>mofucker20</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>That stat is so ridiculous. Whatâ€™s even the us...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>stat ridiculous whats even use home ground hav...</td>\n",
       "      <td>That stat is so ridiculous . What â€™ s even the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>ljubwki</td>\n",
       "      <td>2024-08-25 21:24:17</td>\n",
       "      <td>SquiffyRae</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>Part of the problem is Pindi is such a highway...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>part problem pindi highway road there's always...</td>\n",
       "      <td>Part of the problem is Pindi is such a highway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>ljvk09z</td>\n",
       "      <td>2024-08-26 02:17:08</td>\n",
       "      <td>deleted</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>It's particularly risky for a team like Pakist...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>particularly risky team like pakistan tendency...</td>\n",
       "      <td>It's particularly risky for a team like Pakist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Post_id       Id                 Time          Author  Score  No_comments  \\\n",
       "0  1f0syen  1f0syen  2024-08-25 20:04:21  NoQuestion4045   2445          281   \n",
       "1  1f0syen  lju4ld6  2024-08-25 20:07:28         deleted   1151            0   \n",
       "2  1f0syen  lju9bzy  2024-08-25 20:58:53      mofucker20    289            0   \n",
       "3  1f0syen  ljubwki  2024-08-25 21:24:17      SquiffyRae    173            0   \n",
       "4  1f0syen  ljvk09z  2024-08-26 02:17:08         deleted     48            0   \n",
       "\n",
       "                                                Body     Type  \\\n",
       "0  Bangladesh beats Pakistan for the first ever i...     Post   \n",
       "1  Fort Rawalpindi, yet to be captured, by the ho...  Comment   \n",
       "2  That stat is so ridiculous. Whatâ€™s even the us...  Comment   \n",
       "3  Part of the problem is Pindi is such a highway...  Comment   \n",
       "4  It's particularly risky for a team like Pakist...  Comment   \n",
       "\n",
       "                                  Pre-processed_body  \\\n",
       "0   bangladesh beat pakistan first ever test cricket   \n",
       "1              fort rawalpindi yet capture home side   \n",
       "2  stat ridiculous whats even use home ground hav...   \n",
       "3  part problem pindi highway road there's always...   \n",
       "4  particularly risky team like pakistan tendency...   \n",
       "\n",
       "                            Vader_Pre-processed_body  \n",
       "0  Bangladesh beats Pakistan for the first ever i...  \n",
       "1  Fort Rawalpindi , yet to be captured , by the ...  \n",
       "2  That stat is so ridiculous . What â€™ s even the...  \n",
       "3  Part of the problem is Pindi is such a highway...  \n",
       "4  It's particularly risky for a team like Pakist...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph: Interaction between formats for the most active members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Top N most active members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and sort comment count for non-deleted users\n",
    "\n",
    "# test \n",
    "test_comment_count_by_author = data_frame_test.groupby('Author').size().reset_index(name='Comment/Post_Count')\n",
    "test_comment_count_by_author = test_comment_count_by_author.sort_values(by='Comment/Post_Count', ascending=False)\n",
    "test_comment_count_by_author = test_comment_count_by_author[test_comment_count_by_author['Author'] != \"deleted\"]\n",
    "\n",
    "# # odi\n",
    "# odi_comment_count_by_author = data_frame_odi.groupby('Author').size().reset_index(name='Comment/Post_Count')\n",
    "# odi_comment_count_by_author = odi_comment_count_by_author.sort_values(by='Comment/Post_Count', ascending=False)\n",
    "# odi_comment_count_by_author = odi_comment_count_by_author[odi_comment_count_by_author['Author'] != \"deleted\"]\n",
    "\n",
    "# # t20\n",
    "# t20_comment_count_by_author = data_frame_t20.groupby('Author').size().reset_index(name='Comment/Post_Count')\n",
    "# t20_comment_count_by_author = t20_comment_count_by_author.sort_values(by='Comment/Post_Count', ascending=False)\n",
    "# t20_comment_count_by_author = t20_comment_count_by_author[t20_comment_count_by_author['Author'] != \"deleted\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_count = 50\n",
    "\n",
    "top_n_test_author_list = test_comment_count_by_author['Author'].head(head_count).to_list()\n",
    "# top_n_odi_author_list  = odi_comment_count_by_author['Author'].head(head_count).to_list()\n",
    "# top_n_t20_author_list  = t20_comment_count_by_author['Author'].head(head_count).to_list()\n",
    "\n",
    "top_n_odi_author_list = []\n",
    "top_n_t20_author_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VisRock',\n",
       " 'abettertomorrow47',\n",
       " 'Classymuch',\n",
       " 'crashbandicoochy',\n",
       " 'mongrelbifana']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_test_author_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the post and comments by the top users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prevent some users has barely drop off the top rank, I have included all top users in all top 3 group for filtering\n",
    "top_n_users_set = set()\n",
    "top_n_users_set.update(top_n_test_author_list)\n",
    "top_n_users_set.update(top_n_odi_author_list)\n",
    "top_n_users_set.update(top_n_t20_author_list)\n",
    "\n",
    "to_n_author_content_test = data_frame_test[data_frame_test['Author'].isin(top_n_users_set)]\n",
    "# to_n_author_content_odi  = data_frame_odi[data_frame_odi['Author'].isin(top_n_users_set)]\n",
    "# to_n_author_content_t20  = data_frame_t20[data_frame_t20['Author'].isin(top_n_users_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post_id</th>\n",
       "      <th>Id</th>\n",
       "      <th>Time</th>\n",
       "      <th>Author</th>\n",
       "      <th>Score</th>\n",
       "      <th>No_comments</th>\n",
       "      <th>Body</th>\n",
       "      <th>Type</th>\n",
       "      <th>Pre-processed_body</th>\n",
       "      <th>Vader_Pre-processed_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>ljuphhc</td>\n",
       "      <td>2024-08-25 23:15:35</td>\n",
       "      <td>confused_brown_dude</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>I wasnâ€™t ready for this ðŸ˜­</td>\n",
       "      <td>Comment</td>\n",
       "      <td>wasnt ready</td>\n",
       "      <td>I wasn â€™ t ready for this ðŸ˜­</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>ljuppbl</td>\n",
       "      <td>2024-08-25 23:17:07</td>\n",
       "      <td>confused_brown_dude</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>I canâ€™t get past the idiot thing ðŸ¤£, why does i...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>cant get past idiot thing hit hard</td>\n",
       "      <td>I can â€™ t get past the idiot thing ðŸ¤£ , why doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>ljuv8bq</td>\n",
       "      <td>2024-08-25 23:53:59</td>\n",
       "      <td>Medical_Turing_Test</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>As a Fawad Alam truther it's important to note...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>fawad alam truther important note ss sensational</td>\n",
       "      <td>As a Fawad Alam truther it's important to note...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>ljv3br3</td>\n",
       "      <td>2024-08-26 00:42:41</td>\n",
       "      <td>Medical_Turing_Test</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Probably. But Fawad is ancient.\\n\\nUnlikely. S...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>probably fawad ancient unlikely saim ayub scor...</td>\n",
       "      <td>Probably . But Fawad is ancient . Unlikely . S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1f0syen</td>\n",
       "      <td>ljv8d1c</td>\n",
       "      <td>2024-08-26 01:11:04</td>\n",
       "      <td>Medical_Turing_Test</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.espncricinfo.com/series/quaid-e-az...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>please stop embarrassing</td>\n",
       "      <td>Please stop embarrassing yourself</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Post_id       Id                 Time               Author  Score  \\\n",
       "36  1f0syen  ljuphhc  2024-08-25 23:15:35  confused_brown_dude      5   \n",
       "65  1f0syen  ljuppbl  2024-08-25 23:17:07  confused_brown_dude     30   \n",
       "72  1f0syen  ljuv8bq  2024-08-25 23:53:59  Medical_Turing_Test     12   \n",
       "74  1f0syen  ljv3br3  2024-08-26 00:42:41  Medical_Turing_Test     10   \n",
       "76  1f0syen  ljv8d1c  2024-08-26 01:11:04  Medical_Turing_Test      4   \n",
       "\n",
       "    No_comments                                               Body     Type  \\\n",
       "36            0                          I wasnâ€™t ready for this ðŸ˜­  Comment   \n",
       "65            0  I canâ€™t get past the idiot thing ðŸ¤£, why does i...  Comment   \n",
       "72            0  As a Fawad Alam truther it's important to note...  Comment   \n",
       "74            0  Probably. But Fawad is ancient.\\n\\nUnlikely. S...  Comment   \n",
       "76            0  https://www.espncricinfo.com/series/quaid-e-az...  Comment   \n",
       "\n",
       "                                   Pre-processed_body  \\\n",
       "36                                        wasnt ready   \n",
       "65                 cant get past idiot thing hit hard   \n",
       "72   fawad alam truther important note ss sensational   \n",
       "74  probably fawad ancient unlikely saim ayub scor...   \n",
       "76                           please stop embarrassing   \n",
       "\n",
       "                             Vader_Pre-processed_body  \n",
       "36                        I wasn â€™ t ready for this ðŸ˜­  \n",
       "65  I can â€™ t get past the idiot thing ðŸ¤£ , why doe...  \n",
       "72  As a Fawad Alam truther it's important to note...  \n",
       "74  Probably . But Fawad is ancient . Unlikely . S...  \n",
       "76                  Please stop embarrassing yourself  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_n_author_content_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15105, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_n_author_content_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to import networkx\n",
    "import networkx as nx\n",
    "\n",
    "# here are the three main format\n",
    "test = \"Test\"\n",
    "odi  = \"ODI\"\n",
    "t20  = \"T20\"\n",
    "\n",
    "activity_graph = nx.DiGraph()\n",
    "activity_graph.add_node(test)\n",
    "activity_graph.add_node(odi)\n",
    "activity_graph.add_node(t20)\n",
    "\n",
    "user_check_set = set()\n",
    "\n",
    "# to_n_author_content_test\n",
    "# acc = author comment count\n",
    "# test:\n",
    "acc_test_list = to_n_author_content_test.groupby('Author').size().reset_index(name='Comment/Post_Count')\n",
    "\n",
    "for index, row in acc_test_list.iterrows():\n",
    "    \n",
    "    user = row['Author']\n",
    "    if user not in user_check_set:\n",
    "        user_check_set.add(user)\n",
    "        activity_graph.add_node(user)\n",
    "    \n",
    "    activity_graph.add_edge(user, test, weight=row['Comment/Post_Count'])\n",
    "\n",
    "# # ODI\n",
    "# acc_odi_list = to_n_author_content_odi.groupby('Author').size().reset_index(name='Comment/Post_Count')\n",
    "\n",
    "# for index, row in acc_odi_list.iterrows():\n",
    "    \n",
    "#     user = row['Author']\n",
    "#     if user not in user_check_set:\n",
    "#         user_check_set.add(user)\n",
    "#         activity_graph.add_node(user)\n",
    "    \n",
    "#     activity_graph.add_edge(user, odi, weight=row['Comment/Post_Count'])\n",
    "\n",
    "# # T20\n",
    "# acc_t20_list = to_n_author_content_t20.groupby('Author').size().reset_index(name='Comment/Post_Count')\n",
    "\n",
    "# for index, row in acc_t20_list.iterrows():\n",
    "    \n",
    "#     user = row['Author']\n",
    "#     if user not in user_check_set:\n",
    "#         user_check_set.add(user)\n",
    "#         activity_graph.add_node(user)\n",
    "    \n",
    "#     activity_graph.add_edge(user, t20, weight=row['Comment/Post_Count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.readwrite.write_graphml(activity_graph, 'test_graph.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related community to the top N users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional praw library:\n",
    "from redditClient import redditClient\n",
    "import praw\n",
    "import prawcore\n",
    "import time\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_count = 50\n",
    "\n",
    "top_n_test_author_list = test_comment_count_by_author['Author'].head(head_count).to_list()\n",
    "# top_n_odi_author_list  = odi_comment_count_by_author['Author'].head(head_count).to_list()\n",
    "# top_n_t20_author_list  = t20_comment_count_by_author['Author'].head(head_count).to_list()\n",
    "\n",
    "top_n_odi_author_list = []\n",
    "top_n_t20_author_list = []\n",
    "\n",
    "all_active_user_list = top_n_test_author_list + top_n_odi_author_list + top_n_t20_author_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VisRock',\n",
       " 'abettertomorrow47',\n",
       " 'Classymuch',\n",
       " 'crashbandicoochy',\n",
       " 'mongrelbifana',\n",
       " 'FondantAggravating68',\n",
       " 'MedicalJello2',\n",
       " 'Benny4318',\n",
       " 'NoExplanation6203',\n",
       " 'Tern_Larvidae-2424',\n",
       " 'Frod02000',\n",
       " 'Wazflame',\n",
       " 'Axel292',\n",
       " 'JokesFromTheCrease',\n",
       " 'the_maddest_kiwi',\n",
       " 'TrollerThomas',\n",
       " 'WayToTheDawn63',\n",
       " 'rambo_zaki',\n",
       " 'TheRealMarkChapman',\n",
       " 'MiachealFaraday',\n",
       " 'Outside_Error_7355',\n",
       " 'SuperFaiz21',\n",
       " 'BigV95',\n",
       " 'Jack-sprAt1212',\n",
       " 'sellyme',\n",
       " 'dhun_mohan',\n",
       " 'Upstairs-Farm7106',\n",
       " 'niceguysdofinish1st',\n",
       " 'Ok_Vegetable263',\n",
       " 'Merovech_II',\n",
       " '_rickjames',\n",
       " 'atbg1936',\n",
       " 'Subject-Ordinary6922',\n",
       " 'One_Acanthaceae_1163',\n",
       " 'NoirPochette',\n",
       " 'Nark_Narkins',\n",
       " 'Ghostly_100',\n",
       " 'NotAsOriginal',\n",
       " 'confused_brown_dude',\n",
       " 'maffzlel',\n",
       " 'kiwirish',\n",
       " 'mondognarly_',\n",
       " 'snomanDS',\n",
       " 'Rndomguytf',\n",
       " 'NormalTraining5268',\n",
       " 'GoabNZ',\n",
       " 'Medical_Turing_Test',\n",
       " 'scubadoobidoo',\n",
       " 'Doc8176',\n",
       " 'ViolatingBadgers']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_active_user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cricket\n"
     ]
    }
   ],
   "source": [
    "# construct Reddit client\n",
    "# please update the 2auth verification code if necessary\n",
    "client = redditClient()\n",
    "\n",
    "# load the subreddit weAreMusicMakers\n",
    "subreddit = client.subreddit('Cricket')\n",
    "print(subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redditor VisRock doesn't want their data exposed.\n",
      "Redditor abettertomorrow47 are extracted\n",
      "Redditor Classymuch are extracted\n",
      "Redditor crashbandicoochy are extracted\n",
      "Redditor mongrelbifana are extracted\n",
      "Redditor FondantAggravating68 are extracted\n",
      "Redditor MedicalJello2 are extracted\n",
      "Redditor Benny4318 are extracted\n",
      "Redditor NoExplanation6203 are extracted\n",
      "Redditor Tern_Larvidae-2424 are extracted\n",
      "Redditor Frod02000 are extracted\n",
      "Redditor Wazflame are extracted\n",
      "Redditor Axel292 are extracted\n",
      "Redditor JokesFromTheCrease are extracted\n",
      "Redditor the_maddest_kiwi doesn't want their data exposed.\n",
      "Redditor TrollerThomas are extracted\n",
      "Redditor WayToTheDawn63 are extracted\n",
      "Redditor rambo_zaki are extracted\n",
      "Redditor TheRealMarkChapman are extracted\n",
      "Redditor MiachealFaraday are extracted\n",
      "Redditor Outside_Error_7355 are extracted\n",
      "Redditor SuperFaiz21 are extracted\n",
      "Redditor BigV95 are extracted\n",
      "Redditor Jack-sprAt1212 are extracted\n",
      "Redditor sellyme are extracted\n",
      "Redditor dhun_mohan are extracted\n",
      "Redditor Upstairs-Farm7106 are extracted\n",
      "Redditor niceguysdofinish1st are extracted\n",
      "Redditor Ok_Vegetable263 are extracted\n",
      "Redditor Merovech_II are extracted\n",
      "Redditor _rickjames are extracted\n",
      "Redditor atbg1936 are extracted\n",
      "Redditor Subject-Ordinary6922 are extracted\n",
      "Redditor One_Acanthaceae_1163 are extracted\n",
      "Redditor NoirPochette are extracted\n",
      "Redditor Nark_Narkins are extracted\n",
      "Redditor Ghostly_100 are extracted\n",
      "Redditor NotAsOriginal are extracted\n",
      "Redditor confused_brown_dude are extracted\n",
      "Redditor maffzlel are extracted\n",
      "Redditor kiwirish are extracted\n",
      "Redditor mondognarly_ are extracted\n",
      "Redditor snomanDS are extracted\n",
      "Redditor Rndomguytf are extracted\n",
      "Redditor NormalTraining5268 are extracted\n",
      "Redditor GoabNZ are extracted\n",
      "Redditor Medical_Turing_Test are extracted\n",
      "Redditor scubadoobidoo are extracted\n",
      "Redditor Doc8176 are extracted\n",
      "Redditor ViolatingBadgers are extracted\n"
     ]
    }
   ],
   "source": [
    "import praw.exceptions\n",
    "\n",
    "failed_user_list = []\n",
    "\n",
    "def auto_retry_get_user_community(user: str) -> dict[str, int]:\n",
    "\n",
    "    user_subreddit_count = {}\n",
    "    user_subreddit_set = set()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_info = client.redditor(user)\n",
    "            submissions = user_info.comments.new(limit=1000)\n",
    "            for submission in submissions:\n",
    "                subreddit = submission.subreddit\n",
    "                if subreddit not in user_subreddit_set:\n",
    "                    user_subreddit_set.add(subreddit)\n",
    "                    user_subreddit_count[subreddit] = 0\n",
    "                \n",
    "                user_subreddit_count[subreddit] += 1\n",
    "        except prawcore.exceptions.Forbidden as ex:\n",
    "            print(f\"Redditor {user} doesn't want their data exposed.\")\n",
    "            failed_user_list.append(user)\n",
    "            return None\n",
    "        except prawcore.exceptions.TooManyRequests as ex:\n",
    "            print(\"Rate Limited... Wait for 20 seconds and try again\")\n",
    "            # time.sleep(20)\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    print(f\"Redditor {user} are extracted\")\n",
    "    return user_subreddit_count\n",
    "\n",
    "active_users_activities = {}\n",
    "\n",
    "for user in all_active_user_list:\n",
    "    subreddit_count = auto_retry_get_user_community(user)\n",
    "    if subreddit_count is not None:\n",
    "        active_users_activities[user] = subreddit_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Subreddit(display_name='Cricket'),\n",
       " Subreddit(display_name='CricketAus'),\n",
       " Subreddit(display_name='CricketBuddies'),\n",
       " Subreddit(display_name='EnoughMuskSpam'),\n",
       " Subreddit(display_name='IndiaCricket'),\n",
       " Subreddit(display_name='MumbaiIndians'),\n",
       " Subreddit(display_name='PakCricket'),\n",
       " Subreddit(display_name='WorldTestChampionship'),\n",
       " Subreddit(display_name='cricketworldcup'),\n",
       " Subreddit(display_name='interestingasfuck')}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSubmissionSet = set(allSubmissions)\n",
    "allSubmissionSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cricket': 816,\n",
       " 'srilanka': 75,\n",
       " 'TheDeprogram': 47,\n",
       " 'Asia_irl': 22,\n",
       " 'BatmanArkham': 10,\n",
       " 'MarxistCulture': 10,\n",
       " 'Ben10': 8,\n",
       " 'DeathrattleP**n': 3,\n",
       " 'GoodA**Sub': 2,\n",
       " 'batman': 2,\n",
       " 'ThePenguin': 1}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this tests for getting top 10 most active subreddit that the users was in.\n",
    "# Once this works, I will write a function to apply all other users\n",
    "\n",
    "count_sort_testing = active_users_activities.get('abettertomorrow47')\n",
    "count_sort_testing = dict(sorted(count_sort_testing.items(), key=lambda item: item[1], reverse=True))\n",
    "count_sort_testing_top_10 = {}\n",
    "\n",
    "limit = 0 \n",
    "for key, count in count_sort_testing.items():\n",
    "\n",
    "    # Since we need to show that on presentation,\n",
    "    # let me blind some explicit words\n",
    "    subreddit_name = key.display_name\n",
    "    subreddit_name = subreddit_name.replace(\"Porn\", \"P**n\")\n",
    "    subreddit_name = subreddit_name.replace(\"Ass\", \"A**\")\n",
    "\n",
    "    count_sort_testing_top_10[subreddit_name] = count\n",
    "    if limit >= 10:\n",
    "        break\n",
    "    limit += 1\n",
    "\n",
    "count_sort_testing_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_10_most_active_subreddit_from_user(intput: dict) -> dict[str, int]:\n",
    "    sort_dict = dict(sorted(intput.items(), key=lambda item: item[1], reverse=True))\n",
    "    top_10_most_active_subreddit = {}\n",
    "\n",
    "    limit = 0 \n",
    "    for key, count in sort_dict.items():\n",
    "\n",
    "        # Since we need to show that on presentation,\n",
    "        # let me blind some explicit words\n",
    "        subreddit_name = key.display_name\n",
    "        subreddit_name = subreddit_name.replace(\"Porn\", \"P**n\")\n",
    "        subreddit_name = subreddit_name.replace(\"Ass\", \"A**\")\n",
    "        subreddit_name = f\"r/{subreddit_name}\"\n",
    "\n",
    "        top_10_most_active_subreddit[subreddit_name] = count\n",
    "        if limit >= 6:\n",
    "            break\n",
    "        limit += 1\n",
    "\n",
    "    return top_10_most_active_subreddit\n",
    "\n",
    "top_10_subreddit_per_user = {}\n",
    "\n",
    "for redditor, count_dict in active_users_activities.items():\n",
    "    top_10_most_active_sub_of_the_user = extract_top_10_most_active_subreddit_from_user(count_dict)\n",
    "    top_10_subreddit_per_user[redditor] = top_10_most_active_sub_of_the_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r/Cricket': 927,\n",
       " 'r/Damnthatsinteresting': 21,\n",
       " 'r/Monash': 13,\n",
       " 'r/MadeMeSmile': 5,\n",
       " 'r/computerscience': 4,\n",
       " 'r/CricketBuddies': 4,\n",
       " 'r/Unexpected': 3}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_subreddit_per_user['Classymuch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r/srilanka\n",
      "r/TheDeprogram\n",
      "r/Asia_irl\n",
      "r/BatmanArkham\n",
      "r/MarxistCulture\n",
      "r/Ben10\n",
      "r/Damnthatsinteresting\n",
      "r/Monash\n",
      "r/MadeMeSmile\n",
      "r/computerscience\n",
      "r/CricketBuddies\n",
      "r/Unexpected\n",
      "r/rugbyunion\n",
      "r/kpop\n",
      "r/olympics\n",
      "r/Patriots\n",
      "r/nfl\n",
      "r/nba\n",
      "r/IndiaCricket\n",
      "r/DeathrattleP**n\n",
      "r/SAcricket\n",
      "r/delhicapitals\n",
      "r/olympics\n",
      "r/CricketBuddies\n",
      "r/SquaredCircle\n",
      "r/Guyana\n",
      "r/nba\n",
      "r/Barbados\n",
      "r/cocktails\n",
      "r/AnythingGoesNews\n",
      "r/soccer\n",
      "r/TwoSentenceHorror\n",
      "r/notinteresting\n",
      "r/SAcricket\n",
      "r/Awww\n",
      "r/hypotheticalsituation\n",
      "r/rugbyunion\n",
      "r/newzealand\n",
      "r/chch\n",
      "r/allblacks\n",
      "r/taskmaster\n",
      "r/ufc\n",
      "r/Gunners\n",
      "r/nba\n",
      "r/nfl\n",
      "r/politics\n",
      "r/soccer\n",
      "r/Boxing\n",
      "r/MMA\n",
      "r/singularity\n",
      "r/GeForceNOW\n",
      "r/EnglandCricket\n",
      "r/AskReddit\n",
      "r/FortNiteBR\n",
      "r/SAcricket\n",
      "r/Costco\n",
      "r/crispcricket\n",
      "r/EnglandCricket\n",
      "r/YoungSheldon\n",
      "r/mylittlepony\n",
      "r/thomasthetankengine\n",
      "r/RoleReversal\n",
      "r/DeathInParadiseBBC\n",
      "r/Piratefolk\n",
      "r/OnePiece\n",
      "r/NonPoliticalTwitter\n",
      "r/australia\n",
      "r/BlackPeopleTwitter\n",
      "r/mildlyinfuriating\n",
      "r/reddevils\n",
      "r/soccer\n",
      "r/rugbyunion\n",
      "r/shitposting\n",
      "r/unpopularopinion\n",
      "r/ufc\n",
      "r/springboks\n",
      "r/beatlescirclejerk\n",
      "r/EnoughMuskSpam\n",
      "r/IndiaCricket\n",
      "r/CricketAus\n",
      "r/WorldTestChampionship\n",
      "r/cricketworldcup\n",
      "r/CricketBuddies\n",
      "r/rugbyunion\n",
      "r/ukpolitics\n",
      "r/HousingUK\n",
      "r/unitedkingdom\n",
      "r/badunitedkingdom\n",
      "r/EnglandCricket\n",
      "r/FantasyPL\n",
      "r/reddevils\n",
      "r/soccer\n",
      "r/Gunners\n",
      "r/chelseafc\n",
      "r/LiverpoolFC\n",
      "r/2SriLankan4u\n",
      "r/ElectricalEngineering\n",
      "r/IndianHistory\n",
      "r/ABCDesis\n",
      "r/Buddhism\n",
      "r/fragrance\n",
      "r/soccer\n",
      "r/ThreeLions\n",
      "r/psytrance\n",
      "r/Plastering\n",
      "r/Psybient\n",
      "r/Drugs\n",
      "r/formula1\n",
      "r/2007scape\n",
      "r/baseball\n",
      "r/AFL\n",
      "r/formuladank\n",
      "r/NonPoliticalTwitter\n",
      "r/SquaredCircle\n",
      "r/tennis\n",
      "r/DeathrattleP**n\n",
      "r/okZyox\n",
      "r/WomensCricket\n",
      "r/EnglandCricket\n",
      "r/PakCricket\n",
      "r/McLarenFormula1\n",
      "r/dating_advice\n",
      "r/unitedkingdom\n",
      "r/IndiaCricket\n",
      "r/newzealand\n",
      "r/CricketShitpost\n",
      "r/NameThatSong\n",
      "r/punjabimusic\n",
      "r/cricketworldcup\n",
      "r/TMKOC\n",
      "r/CricketShitpost\n",
      "r/DeathrattleP**n\n",
      "r/unitedkingdom\n",
      "r/footballmanagergames\n",
      "r/Gamingcirclejerk\n",
      "r/EnglandCricket\n",
      "r/soccer\n",
      "r/rugbyunion\n",
      "r/tennis\n",
      "r/soccer\n",
      "r/ukpolitics\n",
      "r/SquaredCircle\n",
      "r/taskmaster\n",
      "r/CasualUK\n",
      "r/padel\n",
      "r/soccer\n",
      "r/chess\n",
      "r/Iceland\n",
      "r/footballmanagergames\n",
      "r/VisitingIceland\n",
      "r/politics\n",
      "r/AustralianPolitics\n",
      "r/australian\n",
      "r/unimelb\n",
      "r/cricketworldcup\n",
      "r/Monash\n",
      "r/rmit\n",
      "r/spotify\n",
      "r/geography\n",
      "r/MovieSuggestions\n",
      "r/MusicRecommendations\n",
      "r/Btechtards\n",
      "r/tolkienfans\n",
      "r/tennis\n",
      "r/DCcomics\n",
      "r/memphisgrizzlies\n",
      "r/movies\n",
      "r/Scoobydoo\n",
      "r/television\n",
      "r/nffc\n",
      "r/rugbyunion\n",
      "r/2westerneurope4u\n",
      "r/nfl\n",
      "r/Jaguars\n",
      "r/soccer\n",
      "r/PakCricket\n",
      "r/ufc\n",
      "r/askasia\n",
      "r/PakistaniFootball\n",
      "r/pakistan\n",
      "r/mmamemes\n",
      "r/rugbyunion\n",
      "r/soccer\n",
      "r/englandrugby\n",
      "r/TorontoRealEstate\n",
      "r/canada\n",
      "r/PakCricket\n",
      "r/NYCapartments\n",
      "r/circlejerknyc\n",
      "r/whatcarshouldIbuy\n",
      "r/DeathrattleP**n\n",
      "r/newzealand\n",
      "r/rugbyunion\n",
      "r/losangeleskings\n",
      "r/CFB\n",
      "r/CasualNZ\n",
      "r/navy\n",
      "r/CasualUK\n",
      "r/offset\n",
      "r/guitarpedals\n",
      "r/TheSimpsons\n",
      "r/unitedkingdom\n",
      "r/guitars\n",
      "r/rugbyunion\n",
      "r/newzealand\n",
      "r/PersonalFinanceNZ\n",
      "r/tabletennis\n",
      "r/LudwigAhgren\n",
      "r/dreamcatcher\n",
      "r/olympics\n",
      "r/worldnews\n",
      "r/hiphopheads\n",
      "r/CricketAus\n",
      "r/popheads\n",
      "r/KendrickLamar\n",
      "r/ipl\n",
      "r/Ni_Bondha\n",
      "r/kuttichevuru\n",
      "r/TwoXIndia\n",
      "r/AskIndia\n",
      "r/indianmemer\n",
      "r/ConservativeKiwi\n",
      "r/chch\n",
      "r/AntiVegan\n",
      "r/AskReddit\n",
      "r/ShitPoliticsSays\n",
      "r/futurama\n",
      "r/Boxing\n",
      "r/rugbyunion\n",
      "r/formuladank\n",
      "r/formula1\n",
      "r/nba\n",
      "r/springboks\n",
      "r/tennis\n",
      "r/AskReddit\n",
      "r/BritishTV\n",
      "r/AlanPartridge\n",
      "r/television\n",
      "r/netflix\n",
      "r/FishingAustralia\n",
      "r/CricketAus\n",
      "r/perth\n",
      "r/CricketBuddies\n",
      "r/DeadlockTheGame\n",
      "r/ShitAmericansSay\n",
      "r/newzealand\n",
      "r/thelastofus\n",
      "r/musicals\n",
      "r/community\n",
      "r/DeathrattleP**n\n",
      "r/AskReddit\n"
     ]
    }
   ],
   "source": [
    "# plot a graph to show the correlation between users and subreddit, attempting to find \n",
    "# other related communities. Count of the subreddit will also be used as weight to reflect \n",
    "# how active other group they are in\n",
    "\n",
    "comm_graph = nx.DiGraph()\n",
    "comm_node_set = set()\n",
    "\n",
    "for redditor, comm_dict in top_10_subreddit_per_user.items():\n",
    "    comm_graph.add_node(redditor)\n",
    "\n",
    "    for comm_name, comm_count in comm_dict.items():\n",
    "        if comm_name not in comm_node_set:\n",
    "            comm_node_set.add(comm_name)\n",
    "            comm_graph.add_node(comm_name)\n",
    "        \n",
    "        if comm_name != 'r/Cricket':\n",
    "            print (comm_name)\n",
    "            comm_graph.add_edge(redditor, comm_name, weight=comm_count)\n",
    "\n",
    "nx.readwrite.write_graphml(comm_graph, '../../Data/Graphs/testing_comm_graph.graphml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
